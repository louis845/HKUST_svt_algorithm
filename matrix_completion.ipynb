{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c71fff3",
   "metadata": {},
   "source": [
    "Important: Each sequence of cells separate by large headers (# in markdown) can be run separately\n",
    "Each sequence of cells under the same large header should be run sequentially from top to bottom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2624ed",
   "metadata": {},
   "source": [
    "# Testing of SVT algorithm with artifical data and given rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46cd0b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import svt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# compute the performance of the prediction\n",
    "# returns the max error in the given locations, the relative error between the Frobenius norm respect to actual matrix,\n",
    "# and the rank.\n",
    "def compute_performance(actual_matrix, predicted_matrix, locations):\n",
    "    diff = []\n",
    "    for k in range(len(locations[0])):\n",
    "        i=locations[0][k]\n",
    "        j=locations[1][k]\n",
    "        diff.append(abs(actual_matrix[i,j] - predicted_matrix[i,j])) \n",
    "    max_diff = max(diff)\n",
    "\n",
    "    # compute the relative error between the actual matrix and the resultant matrix\n",
    "    rel_error = np.linalg.norm(actual_matrix - predicted_matrix, ord='fro') / np.linalg.norm(actual_matrix, ord='fro')\n",
    "\n",
    "    # compute the rank of the predicted matrix\n",
    "    u,s,v = np.linalg.svd(predicted_matrix)\n",
    "    rank = sum(s>0.001)\n",
    "    return max_diff, rel_error, rank\n",
    "    \n",
    "\n",
    "def bulk_test_small_matrices_given_rank(width, height, fixed_entries_num, rank, method = \"normal\", scale = 1.0, num_trials = 5):\n",
    "    max_errors = []\n",
    "    rel_errors = []\n",
    "    ranks = []\n",
    "    time_elapsed = []\n",
    "    for k in range(num_trials):\n",
    "        # generate a matrix with given rank\n",
    "        actual_M = utils.generate_matrix_rank(width, height, rank, method = method, scale = scale)\n",
    "        # generate random locations (of entries) to pass to the sparse matrix\n",
    "        locations = utils.convert_locations( utils.generate_locations(width, height, fixed_entries_num) )\n",
    "        # with the locations, create a sparse matrix\n",
    "        M = utils.filter_locations(actual_M, locations)\n",
    "        \n",
    "        # using SVT algorithm, predict the original matrix from the sparse matrix\n",
    "        time_svt = time.time()\n",
    "        result = svt.svt_algorithm_auto_params_known_rank(M, locations, rank = rank, log=False)\n",
    "        time_svt = int(time.time()-time_svt)\n",
    "        print(\"time elasped: \", time_svt, \" s\")\n",
    "        \n",
    "        max_diff, rel_error, rank = compute_performance(actual_M, result, locations)\n",
    "        print(\"diff on locations:  \",max_diff,\"relative error:  \",rel_error, \"rank:  \", rank)\n",
    "        max_errors.append(max_diff)\n",
    "        rel_errors.append(rel_error)\n",
    "        ranks.append(rank)\n",
    "        time_elapsed.append(time_svt)\n",
    "        \n",
    "    print(\"average absolute error: \", np.mean(np.array(max_errors)),\" average relative error: \", np.mean(np.array(rel_errors)), \" average time elapsed: \", np.mean(np.array(time_elapsed)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36e424",
   "metadata": {},
   "source": [
    "## 1000x1000 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecc9f78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing 1000x1000------------\n",
      "--------Normal, scale=1.0--------\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  11  s\n",
      "diff on locations:   0.046190206429110425 relative error:   0.0016097907925230828 rank:   10\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  10  s\n",
      "diff on locations:   0.04135920168596652 relative error:   0.0016374361589759918 rank:   10\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  11  s\n",
      "diff on locations:   0.054122394551133546 relative error:   0.0016193869092121804 rank:   10\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  11  s\n",
      "diff on locations:   0.040775065526969634 relative error:   0.0016363353470373848 rank:   10\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  11  s\n",
      "diff on locations:   0.04077564975845416 relative error:   0.0015904230845558032 rank:   10\n",
      "average absolute error:  0.04464450359032686  average relative error:  0.0016186744584608885  average time elapsed:  10.8\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  45  s\n",
      "diff on locations:   0.04554152755556906 relative error:   0.0015979740538716628 rank:   50\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  46  s\n",
      "diff on locations:   0.07339129749070228 relative error:   0.001596411371062087 rank:   50\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  46  s\n",
      "diff on locations:   0.04180608256958607 relative error:   0.0015832663102198403 rank:   50\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  43  s\n",
      "diff on locations:   0.049273610988818106 relative error:   0.0015937700125864373 rank:   50\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  44  s\n",
      "diff on locations:   0.057172140908260616 relative error:   0.0015824738302352473 rank:   50\n",
      "average absolute error:  0.05343693190258723  average relative error:  0.0015907791155950549  average time elapsed:  44.8\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  100  s\n",
      "diff on locations:   0.05709806781969462 relative error:   0.0016709053869920516 rank:   100\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  102  s\n",
      "diff on locations:   0.06722694663649964 relative error:   0.001697957985086048 rank:   100\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  102  s\n",
      "diff on locations:   0.05542661662659654 relative error:   0.0016300109226520532 rank:   100\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  102  s\n",
      "diff on locations:   0.06094120463568142 relative error:   0.0016746543984299304 rank:   100\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  103  s\n",
      "diff on locations:   0.052092948970388964 relative error:   0.0016257473482727052 rank:   100\n",
      "average absolute error:  0.058557156937772234  average relative error:  0.0016598552082865577  average time elapsed:  101.8\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Testing 1000x1000------------\")\n",
    "print(\"--------Normal, scale=1.0--------\")\n",
    "bulk_test_small_matrices_given_rank(1000, 1000, 120000, 10, method = \"normal\", scale = 1.0)\n",
    "bulk_test_small_matrices_given_rank(1000, 1000, 390000, 50, method = \"normal\", scale = 1.0)\n",
    "bulk_test_small_matrices_given_rank(1000, 1000, 570000, 100, method = \"normal\", scale = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2079dfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing 1000x1000------------\n",
      "--------Uniform, scale=1.0--------\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  40  s\n",
      "diff on locations:   0.023363620567845134 relative error:   0.0015107345155148306 rank:   10\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  42  s\n",
      "diff on locations:   0.024201306983825388 relative error:   0.0015166416591069116 rank:   10\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  40  s\n",
      "diff on locations:   0.025149492633635973 relative error:   0.0015174344424294674 rank:   10\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  41  s\n",
      "diff on locations:   0.02107851369267255 relative error:   0.0015039401830352695 rank:   10\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  41  s\n",
      "diff on locations:   0.020953770747068168 relative error:   0.0015065301446646469 rank:   10\n",
      "average absolute error:  0.022949340925009443  average relative error:  0.0015110561889502255  average time elapsed:  40.8\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  601  s\n",
      "diff on locations:   0.07496824263516721 relative error:   0.001501037460431406 rank:   50\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  603  s\n",
      "diff on locations:   0.07088480702090294 relative error:   0.0015089877164001944 rank:   50\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  604  s\n",
      "diff on locations:   0.08957560945096077 relative error:   0.001510188312560577 rank:   50\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  605  s\n",
      "diff on locations:   0.0773984754080903 relative error:   0.0015065361390266797 rank:   50\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  597  s\n",
      "diff on locations:   0.06712156437612826 relative error:   0.0015032340668221685 rank:   50\n",
      "average absolute error:  0.0759897397782499  average relative error:  0.001505996739048205  average time elapsed:  602.0\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Testing 1000x1000------------\")\n",
    "print(\"--------Uniform, scale=1.0--------\")\n",
    "bulk_test_small_matrices_given_rank(1000, 1000, 120000, 10, method = \"uniform\", scale = 1.0)\n",
    "bulk_test_small_matrices_given_rank(1000, 1000, 390000, 50, method = \"uniform\", scale = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21a114b",
   "metadata": {},
   "source": [
    "## 5000x5000 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2aa3cb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing 5000x5000------------\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  172  s\n",
      "diff on locations:   0.056700561796979354 relative error:   0.001668984079013127 rank:   10\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  179  s\n",
      "diff on locations:   0.06161376600727131 relative error:   0.0016262885987490623 rank:   10\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  168  s\n",
      "diff on locations:   0.05804636685450504 relative error:   0.0016563405409970307 rank:   10\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  168  s\n",
      "diff on locations:   0.06548888111360007 relative error:   0.0016902205980904444 rank:   10\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  168  s\n",
      "diff on locations:   0.049076109412578006 relative error:   0.0016417363580301817 rank:   10\n",
      "average absolute error:  0.05818513703698676  average relative error:  0.001656714034975969  average time elapsed:  171.0\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  638  s\n",
      "diff on locations:   0.032647412817455645 relative error:   0.001539578491927267 rank:   10\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  681  s\n",
      "diff on locations:   0.024225647259833227 relative error:   0.0015326025778527447 rank:   10\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  668  s\n",
      "diff on locations:   0.026165290264631658 relative error:   0.0015407770165437665 rank:   10\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  672  s\n",
      "diff on locations:   0.028543695442411643 relative error:   0.001535484798217243 rank:   10\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  676  s\n",
      "diff on locations:   0.026643573566596412 relative error:   0.0015186924031012324 rank:   10\n",
      "average absolute error:  0.027645123870185716  average relative error:  0.0015334270575284507  average time elapsed:  667.0\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Testing 5000x5000------------\")\n",
    "bulk_test_small_matrices_given_rank(5000, 5000, 600000, 10, method = \"normal\", scale = 1.0)\n",
    "bulk_test_small_matrices_given_rank(5000, 5000, 600000, 10, method = \"uniform\", scale = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca9d221e",
   "metadata": {},
   "source": [
    "# Comparison of SVT algorithm with gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d8641d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import svt\n",
    "import time\n",
    "import numpy as np\n",
    "import descent\n",
    "\n",
    "# same function as above\n",
    "def compute_performance(actual_matrix, predicted_matrix, locations):\n",
    "    diff = []\n",
    "    for k in range(len(locations[0])):\n",
    "        i=locations[0][k]\n",
    "        j=locations[1][k]\n",
    "        diff.append(abs(actual_matrix[i,j] - predicted_matrix[i,j])) \n",
    "    max_diff = max(diff)\n",
    "\n",
    "    # compute the relative error between the actual matrix and the resultant matrix\n",
    "    rel_error = np.linalg.norm(actual_matrix - predicted_matrix, ord='fro') / np.linalg.norm(actual_matrix, ord='fro')\n",
    "\n",
    "    # compute the rank of the resultant matrix\n",
    "    u,s,v = np.linalg.svd(predicted_matrix)\n",
    "    rank = sum(s>0.001)\n",
    "    return max_diff, rel_error, rank\n",
    "\n",
    "# comparison of SVT with gradient descent\n",
    "def bulk_compare_small_matrices_given_rank(width, height, fixed_entries_num, rank, method = \"normal\", scale = 1.0, num_trials = 5):\n",
    "    max_errors_SVT = []\n",
    "    rel_errors_SVT = []\n",
    "    ranks_SVT = []\n",
    "    time_elapsed_SVT = []\n",
    "    \n",
    "    max_errors_descent = []\n",
    "    rel_errors_descent = []\n",
    "    ranks_descent = []\n",
    "    time_elapsed_descent = []\n",
    "    for k in range(num_trials):\n",
    "        # generate a matrix with given rank\n",
    "        actual_M = utils.generate_matrix_rank(width, height, rank, method = method, scale = scale)\n",
    "        # generate random locations (of entries) to pass to the sparse matrix\n",
    "        locations = utils.convert_locations( utils.generate_locations(width, height, fixed_entries_num) )\n",
    "        # with the locations, create a sparse matrix\n",
    "        M = utils.filter_locations(actual_M, locations)\n",
    "        \n",
    "        # using SVT algorithm, predict the original matrix from the sparse matrix\n",
    "        time_svt = time.time()\n",
    "        result = svt.svt_algorithm_auto_params_known_rank(M, locations, rank = rank, log=False, tolerance = 0.01)\n",
    "        time_svt = int(time.time()-time_svt)\n",
    "        print(\"time elasped: \", time_svt, \" s\")\n",
    "        \n",
    "        # compute the absolute difference of values in entries in the locations,\n",
    "        # for the actual matrix and the resultant matrix\n",
    "        max_diff, rel_error, rank = compute_performance(actual_M, result, locations)\n",
    "        \n",
    "        print(\"diff on locations:  \",max_diff,\"relative error:  \",rel_error, \"rank:  \", rank, \"     (SVT)\")\n",
    "        max_errors_SVT.append(max_diff)\n",
    "        rel_errors_SVT.append(rel_error)\n",
    "        ranks_SVT.append(rank)\n",
    "        time_elapsed_SVT.append(time_svt)\n",
    "        \n",
    "        \n",
    "        # using gradient descent, predict the original matrix from the sparse matrix\n",
    "        time_descent = time.time()\n",
    "        result = descent.gradient_descent_completion(M, locations, rank = rank, log=False, tolerance = 0.01)\n",
    "        time_descent = int(time.time()-time_descent)\n",
    "        print(\"time elasped: \", time_descent, \" s\")\n",
    "        \n",
    "        # compute the absolute difference of values in entries in the locations,\n",
    "        # for the actual matrix and the resultant matrix\n",
    "        max_diff, rel_error, rank = compute_performance(actual_M, result, locations)\n",
    "        \n",
    "        print(\"diff on locations:  \",max_diff,\"relative error:  \",rel_error, \"rank:  \", rank, \"     (descent)\")\n",
    "        max_errors_descent.append(max_diff)\n",
    "        rel_errors_descent.append(rel_error)\n",
    "        ranks_descent.append(rank)\n",
    "        time_elapsed_descent.append(time_descent)\n",
    "    print(\"average absolute error: \", np.mean(np.array(max_errors_SVT)),\" average relative error: \", np.mean(np.array(rel_errors_SVT)), \" average time elapsed: \", np.mean(np.array(time_elapsed_SVT)), \"    (SVT)\")\n",
    "    print(\"average absolute error: \", np.mean(np.array(max_errors_descent)),\" average relative error: \", np.mean(np.array(rel_errors_descent)), \" average time elapsed: \", np.mean(np.array(time_elapsed_descent)), \"    (descent)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58dcf54",
   "metadata": {},
   "source": [
    "## 1000x1000 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b52a50ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing 1000x1000------------\n",
      "--------Normal, scale=1.0--------\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  6  s\n",
      "diff on locations:   0.3698921624713458 relative error:   0.0158726599015314 rank:   10      (SVT)\n",
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 10])\n",
      "time elasped:  284  s\n",
      "diff on locations:   0.4070930164608395 relative error:   0.01594072610270143 rank:   10      (descent)\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  5  s\n",
      "diff on locations:   0.3318315491827324 relative error:   0.015676259758860254 rank:   10      (SVT)\n",
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 10])\n",
      "time elasped:  346  s\n",
      "diff on locations:   0.6246149897106896 relative error:   0.015998973277102813 rank:   10      (descent)\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  5  s\n",
      "diff on locations:   0.3284352142498612 relative error:   0.015343434424884542 rank:   10      (SVT)\n",
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 10])\n",
      "time elasped:  306  s\n",
      "diff on locations:   0.5675267224894345 relative error:   0.01584103185735239 rank:   10      (descent)\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  5  s\n",
      "diff on locations:   0.2937568054109221 relative error:   0.015773527380280424 rank:   10      (SVT)\n",
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 10])\n",
      "time elasped:  267  s\n",
      "diff on locations:   0.566803493723647 relative error:   0.01621523153058051 rank:   10      (descent)\n",
      "Step size:  10.0     tau:  5000\n",
      "time elasped:  5  s\n",
      "diff on locations:   0.399955993977434 relative error:   0.015141996107165928 rank:   10      (SVT)\n",
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 10])\n",
      "time elasped:  255  s\n",
      "diff on locations:   0.5067759014282069 relative error:   0.01620720851599935 rank:   10      (descent)\n",
      "average absolute error:  0.3447743450584591  average relative error:  0.015561575514544509  average time elapsed:  5.2     (SVT)\n",
      "average absolute error:  0.5345628247625636  average relative error:  0.0160406342567473  average time elapsed:  291.6     (descent)\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  22  s\n",
      "diff on locations:   0.5068227697308423 relative error:   0.015715756420879256 rank:   50      (SVT)\n",
      "torch.Size([1000, 50])\n",
      "torch.Size([1000, 50])\n",
      "time elasped:  306  s\n",
      "diff on locations:   0.5853277628537954 relative error:   0.01550979885041224 rank:   50      (descent)\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  23  s\n",
      "diff on locations:   0.40219916957206525 relative error:   0.015336764012669861 rank:   50      (SVT)\n",
      "torch.Size([1000, 50])\n",
      "torch.Size([1000, 50])\n",
      "time elasped:  338  s\n",
      "diff on locations:   0.6207273430812448 relative error:   0.015367825893904556 rank:   50      (descent)\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  22  s\n",
      "diff on locations:   0.4313258540851006 relative error:   0.015590217765844147 rank:   50      (SVT)\n",
      "torch.Size([1000, 50])\n",
      "torch.Size([1000, 50])\n",
      "time elasped:  320  s\n",
      "diff on locations:   0.8303114563066281 relative error:   0.015419133974277995 rank:   50      (descent)\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  24  s\n",
      "diff on locations:   0.4219918404745009 relative error:   0.01486257314624346 rank:   50      (SVT)\n",
      "torch.Size([1000, 50])\n",
      "torch.Size([1000, 50])\n",
      "time elasped:  323  s\n",
      "diff on locations:   0.7397291215182733 relative error:   0.015349972210499104 rank:   50      (descent)\n",
      "Step size:  3.076923076923077     tau:  5000\n",
      "time elasped:  21  s\n",
      "diff on locations:   0.42451139600346366 relative error:   0.01572902451231336 rank:   50      (SVT)\n",
      "torch.Size([1000, 50])\n",
      "torch.Size([1000, 50])\n",
      "time elasped:  294  s\n",
      "diff on locations:   0.49585630621952426 relative error:   0.015632825628572784 rank:   50      (descent)\n",
      "average absolute error:  0.43737020597319454  average relative error:  0.015446867171590018  average time elapsed:  22.4     (SVT)\n",
      "average absolute error:  0.6543903979958932  average relative error:  0.015455911311533335  average time elapsed:  316.2     (descent)\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  50  s\n",
      "diff on locations:   0.551729761109959 relative error:   0.01594875762486706 rank:   100      (SVT)\n",
      "torch.Size([1000, 100])\n",
      "torch.Size([1000, 100])\n",
      "time elasped:  433  s\n",
      "diff on locations:   0.8881436893282886 relative error:   0.015643007510752018 rank:   100      (descent)\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  51  s\n",
      "diff on locations:   0.513831292969698 relative error:   0.015536322119842787 rank:   100      (SVT)\n",
      "torch.Size([1000, 100])\n",
      "torch.Size([1000, 100])\n",
      "time elasped:  526  s\n",
      "diff on locations:   1.1612048852284476 relative error:   0.015171398702580904 rank:   100      (descent)\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  50  s\n",
      "diff on locations:   0.5066809414948739 relative error:   0.015436827839120506 rank:   100      (SVT)\n",
      "torch.Size([1000, 100])\n",
      "torch.Size([1000, 100])\n",
      "time elasped:  545  s\n",
      "diff on locations:   1.1739225737751422 relative error:   0.015252472075549008 rank:   100      (descent)\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  55  s\n",
      "diff on locations:   0.5114538304199405 relative error:   0.0158520947069374 rank:   100      (SVT)\n",
      "torch.Size([1000, 100])\n",
      "torch.Size([1000, 100])\n",
      "time elasped:  475  s\n",
      "diff on locations:   0.75195506293365 relative error:   0.015727270943642367 rank:   100      (descent)\n",
      "Step size:  2.1052631578947367     tau:  5000\n",
      "time elasped:  55  s\n",
      "diff on locations:   0.593906500668016 relative error:   0.01597362214172168 rank:   100      (SVT)\n",
      "torch.Size([1000, 100])\n",
      "torch.Size([1000, 100])\n",
      "time elasped:  436  s\n",
      "diff on locations:   0.7276306851668731 relative error:   0.01587625871368614 rank:   100      (descent)\n",
      "average absolute error:  0.5355204653324975  average relative error:  0.01574952488649789  average time elapsed:  52.2     (SVT)\n",
      "average absolute error:  0.9405713792864804  average relative error:  0.015534081589242087  average time elapsed:  483.0     (descent)\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Testing 1000x1000------------\")\n",
    "print(\"--------Normal, scale=1.0--------\")\n",
    "bulk_compare_small_matrices_given_rank(1000, 1000, 120000, 10, method = \"normal\", scale = 1.0)\n",
    "bulk_compare_small_matrices_given_rank(1000, 1000, 390000, 50, method = \"normal\", scale = 1.0)\n",
    "bulk_compare_small_matrices_given_rank(1000, 1000, 570000, 100, method = \"normal\", scale = 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c97269",
   "metadata": {},
   "source": [
    "## 5000x5000 matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a5a9fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------Testing 5000x5000------------\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  83  s\n",
      "diff on locations:   0.4634906100885061 relative error:   0.015550858205153622 rank:   10      (SVT)\n",
      "torch.Size([5000, 10])\n",
      "torch.Size([5000, 10])\n",
      "time elasped:  2567  s\n",
      "diff on locations:   0.5818949436681358 relative error:   0.01638317606658432 rank:   10      (descent)\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  79  s\n",
      "diff on locations:   0.3316679103173731 relative error:   0.015210209428225154 rank:   10      (SVT)\n",
      "torch.Size([5000, 10])\n",
      "torch.Size([5000, 10])\n",
      "time elasped:  2068  s\n",
      "diff on locations:   0.43889870315511414 relative error:   0.01642233652914515 rank:   10      (descent)\n",
      "Step size:  50.0     tau:  25000\n",
      "time elasped:  99  s\n",
      "diff on locations:   0.37615103489875423 relative error:   0.015918936495274816 rank:   10      (SVT)\n",
      "torch.Size([5000, 10])\n",
      "torch.Size([5000, 10])\n",
      "time elasped:  1892  s\n",
      "diff on locations:   0.5930816483757457 relative error:   0.016483280348564496 rank:   10      (descent)\n",
      "average absolute error:  0.39043651843487776  average relative error:  0.015560001376217865  average time elapsed:  87.0     (SVT)\n",
      "average absolute error:  0.5379584317329985  average relative error:  0.016429597648097993  average time elapsed:  2175.6666666666665     (descent)\n"
     ]
    }
   ],
   "source": [
    "print(\"------------Testing 5000x5000------------\")\n",
    "bulk_compare_small_matrices_given_rank(5000, 5000, 600000, 10, method = \"normal\", scale = 1.0, num_trials = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e5ebe0",
   "metadata": {},
   "source": [
    "# SVT algorithm with logs (large matrices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f00c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step size:  24.0     tau:  50000\n",
      "fro:  15822.868187420558     nuc: 0.0      last rank: 0\n",
      "fro:  15822.868187420558     nuc: 0.0      last rank: 0\n",
      "fro:  15822.868187420558     nuc: 0.0      last rank: 0\n",
      "fro:  15822.868187420558     nuc: 0.0      last rank: 0\n",
      "fro:  11319.030394915182     nuc: 6976.9977332406925      last rank: 44\n",
      "fro:  8090.533476616989     nuc: 11440.343091613897      last rank: 50\n",
      "fro:  5783.231496054918     nuc: 7465.278706156394      last rank: 50\n",
      "fro:  4173.22299476547     nuc: 10646.286710075154      last rank: 50\n",
      "fro:  3237.1991565395356     nuc: 9246.040084572633      last rank: 50\n",
      "fro:  2663.5041970328975     nuc: 10230.11891206503      last rank: 50\n",
      "fro:  2278.388669258814     nuc: 9996.36635876025      last rank: 50\n",
      "fro:  1982.058035480924     nuc: 10269.808748377509      last rank: 50\n",
      "fro:  1741.6146381055107     nuc: 10299.847252400577      last rank: 50\n",
      "fro:  1540.1578739071163     nuc: 10402.922566859954      last rank: 50\n",
      "fro:  1369.3426782972927     nuc: 10459.821753263132      last rank: 50\n",
      "fro:  1223.0108782175237     nuc: 10518.644403191742      last rank: 50\n",
      "fro:  1096.7192318180107     nuc: 10564.96207545232      last rank: 50\n",
      "fro:  986.9755050551599     nuc: 10605.911206307304      last rank: 50\n",
      "fro:  891.0451802782885     nuc: 10640.771802819305      last rank: 50\n",
      "fro:  806.7409717274579     nuc: 10671.09804164616      last rank: 50\n",
      "fro:  732.2994735809414     nuc: 10697.443975584341      last rank: 50\n",
      "fro:  666.2831141788403     nuc: 10720.48123331985      last rank: 50\n",
      "fro:  607.5097650681531     nuc: 10740.689191259175      last rank: 50\n",
      "fro:  554.9991359939772     nuc: 10758.48421200824      last rank: 50\n",
      "fro:  507.93217193805543     nuc: 10774.205101781597      last rank: 50\n",
      "fro:  465.61975085138886     nuc: 10788.136639554366      last rank: 50\n",
      "fro:  427.4783562746227     nuc: 10800.517376514741      last rank: 50\n",
      "fro:  393.0109716756165     nuc: 10811.548903464303      last rank: 50\n",
      "fro:  361.7919449151213     nuc: 10821.40225474305      last rank: 50\n",
      "fro:  333.4548962813415     nuc: 10830.223288447967      last rank: 50\n",
      "fro:  307.68298303432664     nuc: 10838.136928294312      last rank: 50\n",
      "fro:  284.2010040784064     nuc: 10845.250616951402      last rank: 50\n",
      "fro:  262.76895340342423     nuc: 10851.657118960888      last rank: 50\n",
      "fro:  243.1767228958036     nuc: 10857.436817809641      last rank: 50\n",
      "fro:  225.2397235687898     nuc: 10862.659605759924      last rank: 50\n",
      "fro:  208.79524563663722     nuc: 10867.38644773345      last rank: 50\n",
      "fro:  193.69941675859945     nuc: 10871.670682357028      last rank: 50\n",
      "fro:  179.82464747654134     nuc: 10875.559108644113      last rank: 50\n",
      "fro:  167.0574757103819     nuc: 10879.092897776056      last rank: 50\n",
      "fro:  155.29673987663512     nuc: 10882.308362098818      last rank: 50\n",
      "fro:  144.45202400259242     nuc: 10885.23760519633      last rank: 50\n",
      "fro:  134.44232905298827     nuc: 10887.909074337964      last rank: 50\n",
      "fro:  125.1949332520048     nuc: 10890.348031193105      last rank: 50\n",
      "fro:  116.6444109957619     nuc: 10892.576954631935      last rank: 50\n",
      "fro:  108.73178539111832     nuc: 10894.615885817424      last rank: 50\n",
      "fro:  101.40379383104538     nuc: 10896.482725999951      last rank: 50\n",
      "fro:  94.61224955065143     nuc: 10898.193492926453      last rank: 50\n",
      "fro:  88.31348496999877     nuc: 10899.762543098623      last rank: 50\n",
      "fro:  82.46786497563238     nuc: 10901.202764645845      last rank: 50\n",
      "fro:  77.03936019434026     nuc: 10902.525745152823      last rank: 50\n",
      "fro:  71.99517190358003     nuc: 10903.741918171603      last rank: 50\n",
      "fro:  67.30540150598102     nuc: 10904.86069104197      last rank: 50\n",
      "fro:  62.942758585445304     nuc: 10905.890557345832      last rank: 50\n",
      "fro:  58.882302453956925     nuc: 10906.839195366665      last rank: 50\n",
      "fro:  55.101212847864936     nuc: 10907.713554698974      last rank: 50\n",
      "fro:  51.57858606028432     nuc: 10908.519932941133      last rank: 50\n",
      "fro:  48.29525332478362     nuc: 10909.264043281619      last rank: 50\n",
      "fro:  45.23361870859252     nuc: 10909.951074129684      last rank: 50\n",
      "fro:  42.37751415251668     nuc: 10910.585742332143      last rank: 50\n",
      "fro:  39.71206961254608     nuc: 10911.172340363999      last rank: 50\n",
      "fro:  37.22359653097662     nuc: 10911.714778025424      last rank: 50\n",
      "fro:  34.8994830965975     nuc: 10912.21662007847      last rank: 50\n",
      "fro:  32.72809995231925     nuc: 10912.681119429406      last rank: 50\n",
      "fro:  30.69871517846169     nuc: 10913.111246882472      last rank: 50\n",
      "fro:  28.80141752714784     nuc: 10913.509718018497      last rank: 50\n",
      "fro:  27.027047008880103     nuc: 10913.879016964129      last rank: 50\n",
      "fro:  25.367132041662348     nuc: 10914.221417871813      last rank: 50\n",
      "fro:  23.813832467996136     nuc: 10914.53900420644      last rank: 50\n",
      "fro:  22.35988782619187     nuc: 10914.833686065307      last rank: 50\n",
      "fro:  20.998570334770864     nuc: 10915.10721595557      last rank: 50\n",
      "fro:  19.72364210988023     nuc: 10915.361202563254      last rank: 50\n",
      "fro:  18.529316191409784     nuc: 10915.597123835512      last rank: 50\n",
      "fro:  17.410220998618996     nuc: 10915.816338307639      last rank: 50\n",
      "fro:  16.36136788140125     nuc: 10916.020095327956      last rank: 50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time elasped:  2144  s\n",
      "diff on locations:   0.07546449916340414 relative error:   0.0016059550469164096 rank:   50\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import svt\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# same function as above\n",
    "def compute_performance(actual_matrix, predicted_matrix, locations):\n",
    "    diff = []\n",
    "    for k in range(len(locations[0])):\n",
    "        i=locations[0][k]\n",
    "        j=locations[1][k]\n",
    "        diff.append(abs(actual_matrix[i,j] - predicted_matrix[i,j])) \n",
    "    max_diff = max(diff)\n",
    "\n",
    "    # compute the relative error between the actual matrix and the resultant matrix\n",
    "    rel_error = np.linalg.norm(actual_matrix - predicted_matrix, ord='fro') / np.linalg.norm(actual_matrix, ord='fro')\n",
    "\n",
    "    # compute the rank of the predicted matrix\n",
    "    u,s,v = np.linalg.svd(predicted_matrix)\n",
    "    rank = sum(s>0.001)\n",
    "    return max_diff, rel_error, rank\n",
    "\n",
    "def test_single_matrix(width, height, fixed_entries_num, rank, method = \"normal\", scale = 1.0):\n",
    "    # generate a matrix with given rank\n",
    "    actual_M = utils.generate_matrix_rank(width, height, rank, method = method, scale = scale)\n",
    "    # generate random locations (of entries) to pass to the sparse matrix\n",
    "    locations = utils.convert_locations( utils.generate_locations(width, height, fixed_entries_num) )\n",
    "    # with the locations, create a sparse matrix\n",
    "    M = utils.filter_locations(actual_M, locations)\n",
    "\n",
    "    # using SVT algorithm, predict the original matrix from the sparse matrix\n",
    "    time_svt = time.time()\n",
    "    result = svt.svt_algorithm_auto_params_known_rank(M, locations, rank = rank, log=True)\n",
    "    time_svt = int(time.time()-time_svt)\n",
    "    print(\"time elasped: \", time_svt, \" s\")\n",
    "\n",
    "    max_diff, rel_error, rank = compute_performance(actual_M, result, locations)\n",
    "    print(\"diff on locations:  \",max_diff,\"relative error:  \",rel_error, \"rank:  \", rank)\n",
    "\n",
    "test_single_matrix(10000, 10000, 5000000, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759cc1de",
   "metadata": {},
   "source": [
    "# Gradient descent algorithm with logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad44a0b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  40   loss:  1411.8463089160944   quotient loss:  1.2851426895035423\n",
      "epoch:  80   loss:  1324.671940519614   quotient loss:  1.2057916287334545\n",
      "epoch:  120   loss:  1265.331181944079   quotient loss:  1.1517763003005097\n",
      "epoch:  160   loss:  1222.8729492469772   quotient loss:  1.1131284056852586\n",
      "epoch:  200   loss:  1191.2326598947832   quotient loss:  1.0843276174564278\n",
      "epoch:  240   loss:  1166.8206803103894   quotient loss:  1.0621064472758865\n",
      "epoch:  280   loss:  1147.3964550232845   quotient loss:  1.0444254143126335\n",
      "epoch:  320   loss:  1131.4982876699373   quotient loss:  1.0299539995264564\n",
      "epoch:  360   loss:  1118.135837007293   quotient loss:  1.0177907380761835\n",
      "epoch:  400   loss:  1106.6149962837121   quotient loss:  1.0073038145779654\n",
      "epoch:  440   loss:  1096.4344485649838   quotient loss:  0.9980369018883609\n",
      "epoch:  480   loss:  1087.2212286374838   quotient loss:  0.9896505058891357\n",
      "epoch:  520   loss:  1078.6906414723362   quotient loss:  0.9818854809970999\n",
      "epoch:  560   loss:  1070.6194468624228   quotient loss:  0.9745386212978641\n",
      "epoch:  600   loss:  1062.8287726628405   quotient loss:  0.9674471072069404\n",
      "epoch:  640   loss:  1055.1723792821722   quotient loss:  0.9604778231432357\n",
      "epoch:  680   loss:  1047.5287585551052   quotient loss:  0.9535201654742014\n",
      "epoch:  720   loss:  1039.796013648831   quotient loss:  0.9464813819158667\n",
      "epoch:  760   loss:  1031.8888021487587   quotient loss:  0.9392837889558527\n",
      "epoch:  800   loss:  1023.7365383730328   quotient loss:  0.9318631354979547\n",
      "epoch:  840   loss:  1015.2819497558302   quotient loss:  0.9241672888002364\n",
      "epoch:  880   loss:  1006.4814019642887   quotient loss:  0.9161565304149211\n",
      "epoch:  920   loss:  997.3046801253868   quotient loss:  0.9078033570486739\n",
      "epoch:  960   loss:  987.7349720446269   quotient loss:  0.8990924652872943\n",
      "epoch:  1000   loss:  977.7688888484845   quotient loss:  0.8900207703856449\n",
      "epoch:  1040   loss:  967.4161462369749   quotient loss:  0.8805971161256372\n",
      "epoch:  1080   loss:  956.6984242696336   quotient loss:  0.8708412369286765\n",
      "epoch:  1120   loss:  945.6479260274407   quotient loss:  0.8607824458678927\n",
      "epoch:  1160   loss:  934.3050559105415   quotient loss:  0.8504575213228743\n",
      "epoch:  1200   loss:  922.7162348197846   quotient loss:  0.8399087182338295\n",
      "epoch:  1240   loss:  910.9307053777471   quotient loss:  0.8291808600323294\n",
      "epoch:  1280   loss:  898.997983868707   quotient loss:  0.8183190192523667\n",
      "epoch:  1320   loss:  886.9653319042408   quotient loss:  0.807366216096798\n",
      "epoch:  1360   loss:  874.87591977377   quotient loss:  0.796361746614707\n",
      "epoch:  1400   loss:  862.7673498690131   quotient loss:  0.7853398386384857\n",
      "epoch:  1440   loss:  850.6711321068794   quotient loss:  0.7743291742839615\n",
      "epoch:  1480   loss:  838.6123210399428   quotient loss:  0.7633525596277418\n",
      "epoch:  1520   loss:  826.6102089255854   quotient loss:  0.7524275555781085\n",
      "epoch:  1560   loss:  814.6789244849778   quotient loss:  0.7415670228994445\n",
      "epoch:  1600   loss:  802.8282817639149   quotient loss:  0.7307798948935751\n",
      "epoch:  1640   loss:  791.064630735062   quotient loss:  0.7200719516662364\n",
      "epoch:  1680   loss:  779.3918302112231   quotient loss:  0.709446705727986\n",
      "epoch:  1720   loss:  767.8119480054996   quotient loss:  0.6989060393197402\n",
      "epoch:  1760   loss:  756.3258639634109   quotient loss:  0.6884507533268575\n",
      "epoch:  1800   loss:  744.9335121338011   quotient loss:  0.6780807877168503\n",
      "epoch:  1840   loss:  733.6345309484825   quotient loss:  0.6677958133698209\n",
      "epoch:  1880   loss:  722.4281538672202   quotient loss:  0.6575951325373152\n",
      "epoch:  1920   loss:  711.3135296759089   quotient loss:  0.6474779703959139\n",
      "epoch:  1960   loss:  700.2897614559276   quotient loss:  0.6374435105193522\n",
      "epoch:  2000   loss:  689.3559267533717   quotient loss:  0.6274909132376993\n",
      "epoch:  2040   loss:  678.5110767334605   quotient loss:  0.6176193148676536\n",
      "epoch:  2080   loss:  667.7544458856114   quotient loss:  0.6078280185980091\n",
      "epoch:  2120   loss:  657.0852779510434   quotient loss:  0.5981163360390751\n",
      "epoch:  2160   loss:  646.5030935734183   quotient loss:  0.58848383085349\n",
      "epoch:  2200   loss:  636.0075667788867   quotient loss:  0.5789302063213438\n",
      "epoch:  2240   loss:  625.5985883535864   quotient loss:  0.5694553630299769\n",
      "epoch:  2280   loss:  615.2763606705527   quotient loss:  0.5600594851908188\n",
      "epoch:  2320   loss:  605.0412176042224   quotient loss:  0.5507428767153428\n",
      "epoch:  2360   loss:  594.8937405116984   quotient loss:  0.5415060667877993\n",
      "epoch:  2400   loss:  584.8346988679792   quotient loss:  0.5323497558280306\n",
      "epoch:  2440   loss:  574.8649569681561   quotient loss:  0.5232747305665124\n",
      "epoch:  2480   loss:  564.9855086637178   quotient loss:  0.5142818956633107\n",
      "epoch:  2520   loss:  555.1975718156556   quotient loss:  0.5053723596846627\n",
      "epoch:  2560   loss:  545.5025206174578   quotient loss:  0.4965473734995219\n",
      "epoch:  2600   loss:  535.9021307571   quotient loss:  0.4878085534399258\n",
      "epoch:  2640   loss:  526.3987022685371   quotient loss:  0.4791579931273995\n",
      "epoch:  2680   loss:  516.9952248328799   quotient loss:  0.4705984139394748\n",
      "epoch:  2720   loss:  507.6956827175114   quotient loss:  0.46213344258259154\n",
      "epoch:  2760   loss:  498.5051247229059   quotient loss:  0.453767674761664\n",
      "epoch:  2800   loss:  489.4298979424939   quotient loss:  0.4455068879615858\n",
      "epoch:  2840   loss:  480.47777328925423   quotient loss:  0.4373581557086624\n",
      "epoch:  2880   loss:  471.6579467686302   quotient loss:  0.429329848729292\n",
      "epoch:  2920   loss:  462.9810336288086   quotient loss:  0.4214316296252169\n",
      "epoch:  2960   loss:  454.4588127652494   quotient loss:  0.41367422021601147\n",
      "epoch:  3000   loss:  446.104125317173   quotient loss:  0.4060693092358458\n",
      "epoch:  3040   loss:  437.93044895850755   quotient loss:  0.39862916482892996\n",
      "epoch:  3080   loss:  429.9515779422143   quotient loss:  0.39136634330768866\n",
      "epoch:  3120   loss:  422.18114817575645   quotient loss:  0.38429325684948157\n",
      "epoch:  3160   loss:  414.63225423259104   quotient loss:  0.3774218248787138\n",
      "epoch:  3200   loss:  407.3168299984669   quotient loss:  0.3707629102959223\n",
      "epoch:  3240   loss:  400.2454520353729   quotient loss:  0.3643261404884752\n",
      "epoch:  3280   loss:  393.42669266332194   quotient loss:  0.3581193184688726\n",
      "epoch:  3320   loss:  386.8669223712464   quotient loss:  0.3521482430179225\n",
      "epoch:  3360   loss:  380.5698851722243   quotient loss:  0.34641632214895207\n",
      "epoch:  3400   loss:  374.53671382122207   quotient loss:  0.34092458695986105\n",
      "epoch:  3440   loss:  368.7655505195679   quotient loss:  0.33567134637679363\n",
      "epoch:  3480   loss:  363.2516130384007   quotient loss:  0.33065224734345633\n",
      "epoch:  3520   loss:  357.9873644767368   quotient loss:  0.32586042934455123\n",
      "epoch:  3560   loss:  352.9623297888884   quotient loss:  0.3212863573986126\n",
      "epoch:  3600   loss:  348.1635301837342   quotient loss:  0.3169182174728929\n",
      "epoch:  3640   loss:  343.57552390267847   quotient loss:  0.31274195360177665\n",
      "epoch:  3680   loss:  339.18074551041366   quotient loss:  0.3087415767285012\n",
      "epoch:  3720   loss:  334.9596990684103   quotient loss:  0.3048993405426375\n",
      "epoch:  3760   loss:  330.8912798223912   quotient loss:  0.3011960342982978\n",
      "epoch:  3800   loss:  326.9529543757022   quotient loss:  0.2976111468181698\n",
      "epoch:  3840   loss:  323.1211073266493   quotient loss:  0.2941231820225068\n",
      "epoch:  3880   loss:  319.3711716991689   quotient loss:  0.2907097776545306\n",
      "epoch:  3920   loss:  315.6778668595567   quotient loss:  0.28734792184574953\n",
      "epoch:  3960   loss:  312.0154618564599   quotient loss:  0.2840141928229748\n",
      "epoch:  4000   loss:  308.35784812454506   quotient loss:  0.28068482508732734\n",
      "epoch:  4040   loss:  304.67882679963174   quotient loss:  0.2773359709447898\n",
      "epoch:  4080   loss:  300.95223873565055   quotient loss:  0.27394381885502495\n",
      "epoch:  4120   loss:  297.15221267895686   quotient loss:  0.2704848193337299\n",
      "epoch:  4160   loss:  293.25337007867444   quotient loss:  0.26693587138264263\n",
      "epoch:  4200   loss:  289.23111973817754   quotient loss:  0.26327459069805365\n",
      "epoch:  4240   loss:  285.0619039129922   quotient loss:  0.25947953368309223\n",
      "epoch:  4280   loss:  280.72367418869396   quotient loss:  0.25553063061881387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  4320   loss:  276.19616274036105   quotient loss:  0.2514094325799596\n",
      "epoch:  4360   loss:  271.4613951605274   quotient loss:  0.24709957824008244\n",
      "epoch:  4400   loss:  266.5041627254629   quotient loss:  0.24258722375513575\n",
      "epoch:  4440   loss:  261.31246262855507   quotient loss:  0.23786144348889826\n",
      "epoch:  4480   loss:  255.87808376256064   quotient loss:  0.23291476322524615\n",
      "epoch:  4520   loss:  250.19698489390314   quotient loss:  0.22774350440387545\n",
      "epoch:  4560   loss:  244.26975035296533   quotient loss:  0.2223481989154848\n",
      "epoch:  4600   loss:  238.10190996976485   quotient loss:  0.21673388032539673\n",
      "epoch:  4640   loss:  231.7041149289326   quotient loss:  0.21091024394674596\n",
      "epoch:  4680   loss:  225.0921946531021   quotient loss:  0.20489169861897064\n",
      "epoch:  4720   loss:  218.28702792997115   quotient loss:  0.19869724940034517\n",
      "epoch:  4760   loss:  211.31417294516712   quotient loss:  0.19235016080288383\n",
      "epoch:  4800   loss:  204.2033778644467   quotient loss:  0.1858775112964646\n",
      "epoch:  4840   loss:  196.98781489917087   quotient loss:  0.17930949611172692\n",
      "epoch:  4880   loss:  189.7033045310492   quotient loss:  0.1726787210853776\n",
      "epoch:  4920   loss:  182.3872618393346   quotient loss:  0.16601924354736491\n",
      "epoch:  4960   loss:  175.07765018342576   quotient loss:  0.15936561990336287\n",
      "epoch:  5000   loss:  167.8120169918561   quotient loss:  0.15275202795515105\n",
      "epoch:  5040   loss:  160.62641049964355   quotient loss:  0.14621128085343169\n",
      "epoch:  5080   loss:  153.55457600638283   quotient loss:  0.1397740954863004\n",
      "epoch:  5120   loss:  146.62715791250952   quotient loss:  0.13346836612733734\n",
      "epoch:  5160   loss:  139.8711739869942   quotient loss:  0.1273186858842058\n",
      "epoch:  5200   loss:  133.30964307768588   quotient loss:  0.12134600781946453\n",
      "epoch:  5240   loss:  126.9614160754164   quotient loss:  0.11556749108449571\n",
      "epoch:  5280   loss:  120.84112762218003   quotient loss:  0.10999645696154776\n",
      "epoch:  5320   loss:  114.95936782185261   quotient loss:  0.10464254516458331\n",
      "epoch:  5360   loss:  109.32295636810458   quotient loss:  0.0995119633660734\n",
      "epoch:  5400   loss:  103.93527320224352   quotient loss:  0.09460778818054397\n",
      "epoch:  5440   loss:  98.79666618869283   quotient loss:  0.08993033625394847\n",
      "epoch:  5480   loss:  93.90491367601591   quotient loss:  0.08547758531298119\n",
      "epoch:  5520   loss:  89.25562394745779   quotient loss:  0.08124553776764497\n",
      "epoch:  5560   loss:  84.84267955125829   quotient loss:  0.07722862516593595\n",
      "epoch:  5600   loss:  80.65863518363982   quotient loss:  0.07342007036953502\n",
      "epoch:  5640   loss:  76.69504789159303   quotient loss:  0.0698121881231355\n",
      "epoch:  5680   loss:  72.94278847579794   quotient loss:  0.06639666851106668\n",
      "epoch:  5720   loss:  69.39230499261068   quotient loss:  0.06316481681176649\n",
      "epoch:  5760   loss:  66.03385658011919   quotient loss:  0.06010776633953643\n",
      "epoch:  5800   loss:  62.85764469095028   quotient loss:  0.0572165979000953\n",
      "epoch:  5840   loss:  59.85400124339015   quotient loss:  0.054482511056414266\n",
      "epoch:  5880   loss:  57.0134561282466   quotient loss:  0.05189688557729657\n",
      "epoch:  5920   loss:  54.32684424719859   quotient loss:  0.04945137886976202\n",
      "epoch:  5960   loss:  51.78535175365713   quotient loss:  0.04713796807010461\n",
      "epoch:  6000   loss:  49.380547552275075   quotient loss:  0.044948978716535234\n",
      "epoch:  6040   loss:  47.10439961288781   quotient loss:  0.04287709959905692\n",
      "epoch:  6080   loss:  44.94931770460342   quotient loss:  0.04091542165846061\n",
      "epoch:  6120   loss:  42.90810635323251   quotient loss:  0.03905739516550622\n",
      "epoch:  6160   loss:  40.97400269053995   quotient loss:  0.03729686417346104\n",
      "epoch:  6200   loss:  39.14064683133124   quotient loss:  0.03562803955364147\n",
      "epoch:  6240   loss:  37.402057970513965   quotient loss:  0.03404547723760089\n",
      "epoch:  6280   loss:  35.7526256957531   quotient loss:  0.03254407030941513\n",
      "epoch:  6320   loss:  34.187109015703015   quotient loss:  0.031119048121124093\n",
      "epoch:  6360   loss:  32.700582945874224   quotient loss:  0.02976592767215429\n",
      "epoch:  6400   loss:  31.2884626581079   quotient loss:  0.028480535591541938\n",
      "epoch:  6440   loss:  29.946428405049407   quotient loss:  0.02725893979992524\n",
      "epoch:  6480   loss:  28.670460412992476   quotient loss:  0.026097481270992734\n",
      "epoch:  6520   loss:  27.45679534016238   quotient loss:  0.024992734397339875\n",
      "epoch:  6560   loss:  26.301908776235948   quotient loss:  0.023941491060538316\n",
      "epoch:  6600   loss:  25.202500185993454   quotient loss:  0.022940746925992787\n",
      "epoch:  6640   loss:  24.155486930056387   quotient loss:  0.021987696000277054\n",
      "epoch:  6680   loss:  23.157981653369806   quotient loss:  0.02107971004884645\n",
      "epoch:  6720   loss:  22.207270708361555   quotient loss:  0.020214318955572026\n",
      "epoch:  6760   loss:  21.300820290973757   quotient loss:  0.019389216308104897\n",
      "epoch:  6800   loss:  20.436245661049146   quotient loss:  0.01860223138052389\n",
      "epoch:  6840   loss:  19.61131246653717   quotient loss:  0.017851330338703147\n",
      "epoch:  6880   loss:  18.82394760452676   quotient loss:  0.01713462612664094\n",
      "epoch:  6920   loss:  18.07217011096153   quotient loss:  0.016450315558353672\n",
      "epoch:  6960   loss:  17.35413012857548   quotient loss:  0.015796714788703946\n",
      "epoch:  7000   loss:  16.66809096290443   quotient loss:  0.01517224297976312\n",
      "epoch:  7040   loss:  16.012419648754292   quotient loss:  0.014575413713875012\n",
      "epoch:  7080   loss:  15.385597112583644   quotient loss:  0.014004844244033622\n",
      "epoch:  7120   loss:  14.786161933267094   quotient loss:  0.013459204301736126\n",
      "epoch:  7160   loss:  14.212749850074097   quotient loss:  0.012937252059388853\n",
      "epoch:  7200   loss:  13.664060266580721   quotient loss:  0.012437803640265522\n",
      "epoch:  7240   loss:  13.138900516676019   quotient loss:  0.011959773411940119\n",
      "epoch:  7280   loss:  12.636125251711979   quotient loss:  0.011502118813028572\n",
      "epoch:  7320   loss:  12.154645720779206   quotient loss:  0.011063848800622696\n",
      "epoch:  7360   loss:  11.693460994474513   quotient loss:  0.010644052271936662\n",
      "epoch:  7400   loss:  11.251572562617012   quotient loss:  0.010241820326298163\n",
      "torch.Size([1000, 10])\n",
      "torch.Size([1000, 10])\n",
      "time elasped:  321  s\n",
      "diff on locations:   0.44363486111197226 relative error:   0.015873640326570664 rank:   10\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import time\n",
    "import numpy as np\n",
    "import descent\n",
    "\n",
    "# same function as above\n",
    "def compute_performance(actual_matrix, predicted_matrix, locations):\n",
    "    diff = []\n",
    "    for k in range(len(locations[0])):\n",
    "        i=locations[0][k]\n",
    "        j=locations[1][k]\n",
    "        diff.append(abs(actual_matrix[i,j] - predicted_matrix[i,j])) \n",
    "    max_diff = max(diff)\n",
    "\n",
    "    # compute the relative error between the actual matrix and the resultant matrix\n",
    "    rel_error = np.linalg.norm(actual_matrix - predicted_matrix, ord='fro') / np.linalg.norm(actual_matrix, ord='fro')\n",
    "\n",
    "    # compute the rank of the predicted matrix\n",
    "    u,s,v = np.linalg.svd(predicted_matrix)\n",
    "    rank = sum(s>0.001)\n",
    "    return max_diff, rel_error, rank\n",
    "\n",
    "def test_single_matrix(width, height, fixed_entries_num, rank, method = \"normal\", scale = 1.0):\n",
    "    # generate a matrix with given rank\n",
    "    actual_M = utils.generate_matrix_rank(width, height, rank, method = method, scale = scale)\n",
    "    # generate random locations (of entries) to pass to the sparse matrix\n",
    "    locations = utils.convert_locations( utils.generate_locations(width, height, fixed_entries_num) )\n",
    "    # with the locations, create a sparse matrix\n",
    "    M = utils.filter_locations(actual_M, locations)\n",
    "\n",
    "    # using SVT algorithm, predict the original matrix from the sparse matrix\n",
    "    time_svt = time.time()\n",
    "    result = descent.gradient_descent_completion(M, locations, rank = rank, log=True, tolerance = 0.01)\n",
    "    time_svt = int(time.time()-time_svt)\n",
    "    print(\"time elasped: \", time_svt, \" s\")\n",
    "\n",
    "    max_diff, rel_error, rank = compute_performance(actual_M, result, locations)\n",
    "    print(\"diff on locations:  \",max_diff,\"relative error:  \",rel_error, \"rank:  \", rank)\n",
    "\n",
    "test_single_matrix(1000, 1000, 120000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8946fa54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
